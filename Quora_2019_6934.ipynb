{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "#import os\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_uuid": "3b5e47319990386fb96a118fd93ecf194dc43868"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import time\n",
    "import gc\n",
    "import random\n",
    "import os\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "from sklearn.metrics import roc_curve, precision_recall_curve\n",
    "from sklearn.metrics import f1_score, roc_auc_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from textblob import TextBlob\n",
    "import spacy\n",
    "NLP = spacy.load('en')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.optimizer import Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_uuid": "3147c731d0b2424c1e51104aca39633b4fa13fce"
   },
   "outputs": [],
   "source": [
    "def seed_torch(seed=1029):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_uuid": "0d20c4941b6231c205b7cac6a5c3a84e020bfa3c"
   },
   "outputs": [],
   "source": [
    "embed_size = 300 # how big is each word vector\n",
    "max_features = 98000 # how many unique words to use (i.e num rows in embedding vector)\n",
    "maxlen = 45 # max number of words in a question to use\n",
    "batch_size = 2048\n",
    "train_epochs = 4 #5\n",
    "\n",
    "SEED = 1029"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_uuid": "4a4bf98fd37fc6f74d7003e4c1df10b28f0ece68"
   },
   "outputs": [],
   "source": [
    "puncts = [',', '.', '\"', ':', ')', '(', '-', '!', '?', '|', ';', \"'\", '$', '&', '/', '[', ']', '>', '%', '=', '#', '*', '+', '\\\\', '•',  '~', '@', '£', \n",
    " '·', '_', '{', '}', '©', '^', '®', '`',  '<', '→', '°', '€', '™', '›',  '♥', '←', '×', '§', '″', '′', 'Â', '█', '½', 'à', '…', \n",
    " '“', '★', '”', '–', '●', 'â', '►', '−', '¢', '²', '¬', '░', '¶', '↑', '±', '¿', '▾', '═', '¦', '║', '―', '¥', '▓', '—', '‹', '─', \n",
    " '▒', '：', '¼', '⊕', '▼', '▪', '†', '■', '’', '▀', '¨', '▄', '♫', '☆', 'é', '¯', '♦', '¤', '▲', 'è', '¸', '¾', 'Ã', '⋅', '‘', '∞', \n",
    " '∙', '）', '↓', '、', '│', '（', '»', '，', '♪', '╩', '╚', '³', '・', '╦', '╣', '╔', '╗', '▬', '❤', 'ï', 'Ø', '¹', '≤', '‡', '√', ]\n",
    "\n",
    "def clean_text(x):\n",
    "    x = str(x)\n",
    "    for punct in puncts:\n",
    "        if punct in x: #speed up\n",
    "            x = x.replace(punct, ' {} '.format(punct))\n",
    "    return x\n",
    "\n",
    "def Sp_Tokenizer(text): \n",
    "    #Tokenize the text\n",
    "    return [tok.text for tok in NLP.tokenizer(text)]\n",
    "\n",
    "def clean_numbers(x):\n",
    "    x = re.sub('[0-9]{5,}', '#####', x)\n",
    "    x = re.sub('[0-9]{4}', '####', x)\n",
    "    x = re.sub('[0-9]{3}', '###', x)\n",
    "    x = re.sub('[0-9]{2}', '##', x)\n",
    "    x = re.sub(u'\\u200b', '', x) # \\u200b\n",
    "    x = re.sub(r'([A-Za-z])\\1{3,}', r'\\1', x) # same char more than 3 times\n",
    "    return x\n",
    "\n",
    "mispell_dict = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\", \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\", \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",  \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\", \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\", \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\", \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\", \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\", \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",  \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\", \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\", \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\", 'colour': 'color', 'centre': 'center', 'favourite': 'favorite', 'travelling': 'traveling', 'counselling': 'counseling', 'theatre': 'theater', 'cancelled': 'canceled', 'labour': 'labor', 'organisation': 'organization', 'wwii': 'world war 2', 'citicise': 'criticize', 'youtu ': 'youtube ', 'Qoura': 'Quora', 'sallary': 'salary', 'Whta': 'What', 'narcisist': 'narcissist', 'howdo': 'how do', 'whatare': 'what are', 'howcan': 'how can', 'howmuch': 'how much', 'howmany': 'how many', 'whydo': 'why do', 'doI': 'do I', 'theBest': 'the best', 'howdoes': 'how does', 'mastrubation': 'masturbation', 'mastrubate': 'masturbate', \"mastrubating\": 'masturbating', 'pennis': 'penis', 'Etherium': 'Ethereum', 'narcissit': 'narcissist', 'bigdata': 'big data', '2k17': '2017', '2k18': '2018', 'qouta': 'quota', 'exboyfriend': 'ex boyfriend', 'airhostess': 'air hostess', \"whst\": 'what', 'watsapp': 'whatsapp', 'demonitisation': 'demonetization', 'demonitization': 'demonetization', 'demonetisation': 'demonetization'}\n",
    "\n",
    "def _get_mispell(mispell_dict):\n",
    "    mispell_re = re.compile('(%s)' % '|'.join(mispell_dict.keys()))\n",
    "    return mispell_dict, mispell_re\n",
    "\n",
    "mispellings, mispellings_re = _get_mispell(mispell_dict)\n",
    "def replace_typical_misspell(text):\n",
    "    def replace(match):\n",
    "        return mispellings[match.group(0)]\n",
    "    return mispellings_re.sub(replace, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_uuid": "ff7958ba6a5e33f76c6b8f1d730926feccc708cb"
   },
   "outputs": [],
   "source": [
    "def add_features(df):\n",
    "    \n",
    "    df['question_text'] = df['question_text'].progress_apply(lambda x:str(x))\n",
    "    df['total_length'] = df['question_text'].progress_apply(len)\n",
    "    df['capitals'] = df['question_text'].progress_apply(lambda comment: sum(1 for c in comment if c.isupper()))\n",
    "    df['caps_vs_length'] = df.progress_apply(lambda row: float(row['capitals'])/float(row['total_length']),\n",
    "                                axis=1)\n",
    "    df['num_words'] = df.question_text.str.count('\\S+')\n",
    "    df['num_unique_words'] = df['question_text'].progress_apply(lambda comment: len(set(w for w in comment.split())))\n",
    "    df['words_vs_unique'] = df['num_unique_words'] / df['num_words']  \n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_uuid": "da7db8df95c127af04ffe8816f1de84704a5d5ae"
   },
   "outputs": [],
   "source": [
    "def load_and_prec():\n",
    "    ## reading\n",
    "    train_df = pd.read_csv(\"../input/train.csv\")\n",
    "    test_df = pd.read_csv(\"../input/test.csv\")\n",
    "    ##test_df = pd.concat([test_df]*7) # 压力测试\n",
    "    train_df[\"question_text\"] = train_df[\"question_text\"].progress_apply(lambda x: str(x))\n",
    "    test_df[\"question_text\"] = test_df[\"question_text\"].progress_apply(lambda x: str(x))\n",
    "    print(\"Train shape : \",train_df.shape)\n",
    "    print(\"Test shape : \",test_df.shape)\n",
    "    \n",
    "    ## aux before token\n",
    "    train_df['init_len'] = train_df[\"question_text\"].progress_apply(len) # initial total len\n",
    "    test_df['init_len'] = test_df[\"question_text\"].progress_apply(len)\n",
    "    \n",
    "    train_df['init_words'] = train_df[\"question_text\"].progress_apply(lambda x: len(x.split(' '))) # initial total words\n",
    "    test_df['init_words'] = test_df[\"question_text\"].progress_apply(lambda x: len(x.split(' ')))\n",
    "    \n",
    "    train_df['caps'] = train_df['question_text'].progress_apply(lambda comment: sum(1 for c in comment if c.isupper())) # initial upper\n",
    "    test_df['caps'] = test_df['question_text'].progress_apply(lambda comment: sum(1 for c in comment if c.isupper()))\n",
    "\n",
    "    train_df['nums'] = train_df['question_text'].progress_apply(lambda comment: sum(1 for c in comment if c.isdecimal())) # initial num\n",
    "    test_df['nums'] = test_df['question_text'].progress_apply(lambda comment: sum(1 for c in comment if c.isdecimal()))\n",
    "    \n",
    "    train_df['symbols'] = train_df['question_text'].progress_apply(lambda x: len(''.join(e for e in x if not e.isalnum())) - len(''.join(e for e in x if x==' '))) # initial symbols\n",
    "    test_df['symbols'] = test_df['question_text'].progress_apply(lambda x: len(''.join(e for e in x if not e.isalnum())) - len(''.join(e for e in x if x==' '))) \n",
    "    \n",
    "    train_df['caps_ratio'] = train_df.progress_apply(lambda row: float(row['caps'])/float(row['init_len']), axis=1) # upper ratio\n",
    "    test_df['caps_ratio'] = test_df.progress_apply(lambda row: float(row['caps'])/float(row['init_len']), axis=1)\n",
    "    \n",
    "    train_df['nums_ratio'] = train_df.progress_apply(lambda row: float(row['nums'])/float(row['init_len']), axis=1) # upper ratio\n",
    "    test_df['nums_ratio'] = test_df.progress_apply(lambda row: float(row['nums'])/float(row['init_len']), axis=1)\n",
    "\n",
    "    train_df['symbol_ratio'] = train_df.progress_apply(lambda row: float(row['symbols'])/float(row['init_len']), axis=1) # symbols ratio\n",
    "    test_df['symbol_ratio'] = test_df.progress_apply(lambda row: float(row['symbols'])/float(row['init_len']), axis=1)\n",
    "\n",
    "    train_df['word_len_max'] = train_df['question_text'].progress_apply(lambda x: max(len(c) for c in x)) # longest word\n",
    "    test_df['word_len_max'] = test_df['question_text'].progress_apply(lambda x: max(len(c) for c in x))\n",
    "\n",
    "    train_df['len_ratio'] = train_df.progress_apply(lambda row: float(row['word_len_max'])/float(row['init_len']), axis=1) # longest word ratio\n",
    "    test_df['len_ratio'] = test_df.progress_apply(lambda row: float(row['word_len_max'])/float(row['init_len']), axis=1)\n",
    "    \n",
    "    features = train_df[['init_len', 'init_words', 'caps', 'nums', 'symbols', 'caps_ratio',\n",
    "                         'nums_ratio', 'symbol_ratio', 'word_len_max', 'len_ratio']].fillna(0)\n",
    "    test_features = test_df[['init_len', 'init_words', 'caps', 'nums', 'symbols', 'caps_ratio',\n",
    "                             'nums_ratio', 'symbol_ratio', 'word_len_max', 'len_ratio']].fillna(0)\n",
    "    \n",
    "    # scale\n",
    "    ss = StandardScaler()\n",
    "    for i in features.columns:\n",
    "        ss.fit(np.vstack((features[[i]], test_features[[i]])))\n",
    "        features[i] = ss.transform(features[[i]].values)\n",
    "        test_features[i] = ss.transform(test_features[[i]].values)\n",
    "    \n",
    "    features = features.values\n",
    "    test_features = test_features.values\n",
    "    \n",
    "    ## lower    \n",
    "    train_df[\"question_text\"] = train_df[\"question_text\"].progress_apply(lambda x: x.lower())\n",
    "    test_df[\"question_text\"] = test_df[\"question_text\"].progress_apply(lambda x: x.lower())\n",
    "    \n",
    "    ## Clean the text\n",
    "    train_df[\"question_text\"] = train_df[\"question_text\"].progress_apply(lambda x: clean_text(x))\n",
    "    test_df[\"question_text\"] = test_df[\"question_text\"].progress_apply(lambda x: clean_text(x))\n",
    "    \n",
    "    ## Clean numbers\n",
    "    train_df[\"question_text\"] = train_df[\"question_text\"].progress_apply(lambda x: clean_numbers(x))\n",
    "    test_df[\"question_text\"] = test_df[\"question_text\"].progress_apply(lambda x: clean_numbers(x))\n",
    "    \n",
    "    ## Clean speelings\n",
    "    train_df[\"question_text\"] = train_df[\"question_text\"].progress_apply(lambda x: replace_typical_misspell(x))\n",
    "    test_df[\"question_text\"] = test_df[\"question_text\"].progress_apply(lambda x: replace_typical_misspell(x))\n",
    "    \n",
    "    ## Fill up the missing values\n",
    "    train_X = train_df[\"question_text\"].fillna(\"_##_\")\n",
    "    test_X = test_df[\"question_text\"].fillna(\"_##_\")    \n",
    "\n",
    "    ## Tokenize the sentences\n",
    "    train_vocab = train_X.apply(Sp_Tokenizer)\n",
    "    tokenizer = Tokenizer(filters='', lower=False, num_words=max_features)\n",
    "    tokenizer.fit_on_texts(train_vocab)\n",
    "    del train_vocab\n",
    "    gc.collect()\n",
    "\n",
    "    train_X = train_X.values\n",
    "    test_X = test_X.values\n",
    "    \n",
    "    #tokenizer = Tokenizer(num_words=max_features)\n",
    "    #tokenizer.fit_on_texts(list(train_X))\n",
    "    train_X = tokenizer.texts_to_sequences(train_X)\n",
    "    test_X = tokenizer.texts_to_sequences(test_X)\n",
    "\n",
    "    ## Pad the sentences \n",
    "    train_X = pad_sequences(train_X, maxlen=maxlen)\n",
    "    test_X = pad_sequences(test_X, maxlen=maxlen)\n",
    "\n",
    "    ## Get the target values\n",
    "    train_y = train_df['target'].values\n",
    "    \n",
    "    #shuffling the data\n",
    "    np.random.seed(SEED)\n",
    "    trn_idx = np.random.permutation(len(train_X))\n",
    "\n",
    "    train_X = train_X[trn_idx]\n",
    "    train_y = train_y[trn_idx]\n",
    "    features = features[trn_idx] #\n",
    "    #return train_X, test_X, train_y, tokenizer.word_index\n",
    "    return train_X, test_X, train_y, features, test_features, tokenizer.word_index\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_uuid": "111300ec2b632d48189b37d1ac30d10978aceb67"
   },
   "outputs": [],
   "source": [
    "def load_glove(word_index):\n",
    "    EMBEDDING_FILE = '../input/embeddings/glove.840B.300d/glove.840B.300d.txt'\n",
    "    def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\n",
    "    embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(EMBEDDING_FILE))\n",
    "\n",
    "    all_embs = np.stack(embeddings_index.values())\n",
    "    emb_mean,emb_std = all_embs.mean(), all_embs.std()\n",
    "    embed_size = all_embs.shape[1]\n",
    "\n",
    "    # word_index = tokenizer.word_index\n",
    "    nb_words = min(max_features, len(word_index)) + 1 # keras word_index start from 1\n",
    "    embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n",
    "    for word, i in word_index.items():\n",
    "        if i >= max_features: continue\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None: embedding_matrix[i] = embedding_vector\n",
    "    \n",
    "    embedding_matrix[0] = np.zeros(embed_size,)\n",
    "    return embedding_matrix \n",
    "\n",
    "def load_para(word_index):\n",
    "    EMBEDDING_FILE = '../input/embeddings/paragram_300_sl999/paragram_300_sl999.txt'\n",
    "    def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\n",
    "    embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(EMBEDDING_FILE, encoding=\"utf8\", errors='ignore') if len(o)>100)\n",
    "\n",
    "    all_embs = np.stack(embeddings_index.values())\n",
    "    emb_mean,emb_std = all_embs.mean(), all_embs.std()\n",
    "    embed_size = all_embs.shape[1]\n",
    "\n",
    "    # word_index = tokenizer.word_index\n",
    "    nb_words = min(max_features, len(word_index)) + 1 # keras word_index start from 1\n",
    "    embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n",
    "    for word, i in word_index.items():\n",
    "        if i >= max_features: continue\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None: embedding_matrix[i] = embedding_vector\n",
    "    \n",
    "    embedding_matrix[0] = np.zeros(embed_size,)\n",
    "    return embedding_matrix\n",
    "\n",
    "def load_fasttext(word_index):    \n",
    "    EMBEDDING_FILE = '../input/embeddings/wiki-news-300d-1M/wiki-news-300d-1M.vec'\n",
    "    def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\n",
    "    embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(EMBEDDING_FILE, encoding=\"utf8\", errors='ignore') if len(o)>100)\n",
    "\n",
    "    all_embs = np.stack(embeddings_index.values())\n",
    "    emb_mean,emb_std = all_embs.mean(), all_embs.std()\n",
    "    embed_size = all_embs.shape[1]\n",
    "\n",
    "    # word_index = tokenizer.word_index\n",
    "    nb_words = min(max_features, len(word_index)) + 1 # keras word_index start from 1\n",
    "    embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n",
    "    for word, i in word_index.items():\n",
    "        if i >= max_features: continue\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None: embedding_matrix[i] = embedding_vector\n",
    "    \n",
    "    embedding_matrix[0] = np.zeros(embed_size,)\n",
    "    return embedding_matrix  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_uuid": "0bf5c75cbf61c53b4f623f0053fdeac1caee6508"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1306122/1306122 [00:01<00:00, 783878.59it/s]\n",
      "100%|██████████| 56370/56370 [00:00<00:00, 775904.74it/s]\n",
      "  4%|▍         | 54843/1306122 [00:00<00:02, 548425.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape :  (1306122, 3)\n",
      "Test shape :  (56370, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1306122/1306122 [00:01<00:00, 812116.08it/s] \n",
      "100%|██████████| 56370/56370 [00:00<00:00, 804760.21it/s]\n",
      "100%|██████████| 1306122/1306122 [00:02<00:00, 493348.43it/s]\n",
      "100%|██████████| 56370/56370 [00:00<00:00, 451762.21it/s]\n",
      "100%|██████████| 1306122/1306122 [00:09<00:00, 142541.30it/s]\n",
      "100%|██████████| 56370/56370 [00:00<00:00, 141389.23it/s]\n",
      "100%|██████████| 1306122/1306122 [00:08<00:00, 146593.69it/s]\n",
      "100%|██████████| 56370/56370 [00:00<00:00, 144683.21it/s]\n",
      "100%|██████████| 1306122/1306122 [00:13<00:00, 95522.52it/s]\n",
      "100%|██████████| 56370/56370 [00:00<00:00, 92977.05it/s]\n",
      "100%|██████████| 1306122/1306122 [00:34<00:00, 37610.79it/s]\n",
      "100%|██████████| 56370/56370 [00:01<00:00, 39353.98it/s]\n",
      "100%|██████████| 1306122/1306122 [00:34<00:00, 38236.17it/s]\n",
      "100%|██████████| 56370/56370 [00:01<00:00, 37271.07it/s]\n",
      "100%|██████████| 1306122/1306122 [00:34<00:00, 38080.61it/s]\n",
      "100%|██████████| 56370/56370 [00:01<00:00, 38238.91it/s]\n",
      "100%|██████████| 1306122/1306122 [00:13<00:00, 100069.89it/s]\n",
      "100%|██████████| 56370/56370 [00:00<00:00, 99866.62it/s] \n",
      "100%|██████████| 1306122/1306122 [00:34<00:00, 38203.00it/s]\n",
      "100%|██████████| 56370/56370 [00:01<00:00, 38353.49it/s]\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/utils/validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/utils/validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/utils/validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/utils/validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/utils/validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/utils/validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/utils/validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/utils/validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/utils/validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/utils/validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/utils/validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/utils/validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/utils/validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/utils/validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/utils/validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/utils/validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/utils/validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/utils/validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "100%|██████████| 1306122/1306122 [00:01<00:00, 748218.22it/s]\n",
      "100%|██████████| 56370/56370 [00:00<00:00, 694180.20it/s]\n",
      "100%|██████████| 1306122/1306122 [00:08<00:00, 149363.54it/s]\n",
      "100%|██████████| 56370/56370 [00:00<00:00, 149842.05it/s]\n",
      "100%|██████████| 1306122/1306122 [00:24<00:00, 52914.29it/s]\n",
      "100%|██████████| 56370/56370 [00:01<00:00, 53221.07it/s]\n",
      "100%|██████████| 1306122/1306122 [00:35<00:00, 36533.14it/s]\n",
      "100%|██████████| 56370/56370 [00:01<00:00, 36333.28it/s]\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:6: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "  \n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:26: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Took 13.53 minutes\n",
      "(98001, 600)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "train_X, test_X, train_y, features, test_features, word_index = load_and_prec()\n",
    "embedding_matrix_1 = load_glove(word_index)\n",
    "embedding_matrix_2 = load_para(word_index)\n",
    "###embedding_matrix_3 = load_fasttext(word_index)\n",
    "total_time = (time.time() - start_time) / 60\n",
    "print(\"Took {:.2f} minutes\".format(total_time))\n",
    "\n",
    "#embedding_matrix = np.mean([embedding_matrix_1, embedding_matrix_2], axis=0)\n",
    "###embedding_matrix = np.concatenate((embedding_matrix_1, embedding_matrix_2,embedding_matrix_3), axis=1)\n",
    "embedding_matrix = np.concatenate((embedding_matrix_1, embedding_matrix_2), axis=1)###\n",
    "#embedding_matrix = 0.6*embedding_matrix_1+0.4*embedding_matrix_2\n",
    "print(np.shape(embedding_matrix))\n",
    "\n",
    "del embedding_matrix_1, embedding_matrix_2###,embedding_matrix_3\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_uuid": "c974dd099d04d771b1e8cbf8d3a302d62f8296e0"
   },
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, feature_dim, step_dim, bias=True, **kwargs):\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "        \n",
    "        self.supports_masking = True\n",
    "\n",
    "        self.bias = bias\n",
    "        self.feature_dim = feature_dim\n",
    "        self.step_dim = step_dim\n",
    "        self.features_dim = 0\n",
    "        \n",
    "        weight = torch.zeros(feature_dim, 1)\n",
    "        nn.init.xavier_uniform_(weight)\n",
    "        self.weight = nn.Parameter(weight)\n",
    "        \n",
    "        if bias:\n",
    "            self.b = nn.Parameter(torch.zeros(step_dim))\n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        feature_dim = self.feature_dim\n",
    "        step_dim = self.step_dim\n",
    "\n",
    "        eij = torch.mm(\n",
    "            x.contiguous().view(-1, feature_dim), \n",
    "            self.weight\n",
    "        ).view(-1, step_dim)\n",
    "        \n",
    "        if self.bias:\n",
    "            eij = eij + self.b\n",
    "            \n",
    "        eij = torch.tanh(eij)\n",
    "        a = torch.exp(eij)\n",
    "        \n",
    "        if mask is not None:\n",
    "            a = a * mask\n",
    "\n",
    "        a = a / torch.sum(a, 1, keepdim=True) + 1e-10\n",
    "\n",
    "        weighted_input = x * torch.unsqueeze(a, -1)\n",
    "        return torch.sum(weighted_input, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_uuid": "dd445000fa18fb6aab0f584c71ea1e3c1cc3a2cd"
   },
   "outputs": [],
   "source": [
    "class CyclicLR(object):\n",
    "    def __init__(self, optimizer, base_lr=1e-3, max_lr=6e-3,\n",
    "                 step_size=2000, mode='triangular', gamma=1.,\n",
    "                 scale_fn=None, scale_mode='cycle', last_batch_iteration=-1):\n",
    "\n",
    "        if not isinstance(optimizer, Optimizer):\n",
    "            raise TypeError('{} is not an Optimizer'.format(\n",
    "                type(optimizer).__name__))\n",
    "        self.optimizer = optimizer\n",
    "\n",
    "        if isinstance(base_lr, list) or isinstance(base_lr, tuple):\n",
    "            if len(base_lr) != len(optimizer.param_groups):\n",
    "                raise ValueError(\"expected {} base_lr, got {}\".format(\n",
    "                    len(optimizer.param_groups), len(base_lr)))\n",
    "            self.base_lrs = list(base_lr)\n",
    "        else:\n",
    "            self.base_lrs = [base_lr] * len(optimizer.param_groups)\n",
    "\n",
    "        if isinstance(max_lr, list) or isinstance(max_lr, tuple):\n",
    "            if len(max_lr) != len(optimizer.param_groups):\n",
    "                raise ValueError(\"expected {} max_lr, got {}\".format(\n",
    "                    len(optimizer.param_groups), len(max_lr)))\n",
    "            self.max_lrs = list(max_lr)\n",
    "        else:\n",
    "            self.max_lrs = [max_lr] * len(optimizer.param_groups)\n",
    "\n",
    "        self.step_size = step_size\n",
    "\n",
    "        if mode not in ['triangular', 'triangular2', 'exp_range'] \\\n",
    "                and scale_fn is None:\n",
    "            raise ValueError('mode is invalid and scale_fn is None')\n",
    "\n",
    "        self.mode = mode\n",
    "        self.gamma = gamma\n",
    "\n",
    "        if scale_fn is None:\n",
    "            if self.mode == 'triangular':\n",
    "                self.scale_fn = self._triangular_scale_fn\n",
    "                self.scale_mode = 'cycle'\n",
    "            elif self.mode == 'triangular2':\n",
    "                self.scale_fn = self._triangular2_scale_fn\n",
    "                self.scale_mode = 'cycle'\n",
    "            elif self.mode == 'exp_range':\n",
    "                self.scale_fn = self._exp_range_scale_fn\n",
    "                self.scale_mode = 'iterations'\n",
    "        else:\n",
    "            self.scale_fn = scale_fn\n",
    "            self.scale_mode = scale_mode\n",
    "\n",
    "        self.batch_step(last_batch_iteration + 1)\n",
    "        self.last_batch_iteration = last_batch_iteration\n",
    "\n",
    "    def batch_step(self, batch_iteration=None):\n",
    "        if batch_iteration is None:\n",
    "            batch_iteration = self.last_batch_iteration + 1\n",
    "        self.last_batch_iteration = batch_iteration\n",
    "        for param_group, lr in zip(self.optimizer.param_groups, self.get_lr()):\n",
    "            param_group['lr'] = lr\n",
    "\n",
    "    def _triangular_scale_fn(self, x):\n",
    "        return 1.\n",
    "\n",
    "    def _triangular2_scale_fn(self, x):\n",
    "        return 1 / (2. ** (x - 1))\n",
    "\n",
    "    def _exp_range_scale_fn(self, x):\n",
    "        return self.gamma**(x)\n",
    "\n",
    "    def get_lr(self):\n",
    "        step_size = float(self.step_size)\n",
    "        cycle = np.floor(1 + self.last_batch_iteration / (2 * step_size))\n",
    "        x = np.abs(self.last_batch_iteration / step_size - 2 * cycle + 1)\n",
    "\n",
    "        lrs = []\n",
    "        param_lrs = zip(self.optimizer.param_groups, self.base_lrs, self.max_lrs)\n",
    "        for param_group, base_lr, max_lr in param_lrs:\n",
    "            base_height = (max_lr - base_lr) * np.maximum(0, (1 - x))\n",
    "            if self.scale_mode == 'cycle':\n",
    "                lr = base_lr + base_height * self.scale_fn(cycle)\n",
    "            else:\n",
    "                lr = base_lr + base_height * self.scale_fn(self.last_batch_iteration)\n",
    "            lrs.append(lr)\n",
    "        return lrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_uuid": "f02e574e7e72b0aaa6e7609146cfb2dcd2c2e5dd"
   },
   "outputs": [],
   "source": [
    "splits = list(StratifiedKFold(n_splits=5, shuffle=True, random_state=SEED).split(train_X, train_y)) #5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_uuid": "b3f093d480842afd6fe44ca55ca10498f38486a7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef sigmoid(x):\\n    return .5 * (1 + np.tanh(.5 * x))\\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "'''\n",
    "def sigmoid(x):\n",
    "    return .5 * (1 + np.tanh(.5 * x))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_uuid": "cd218973aa94c59de481947dafe582b534dcac0e"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "def threshold_search(y_true, y_proba):\n",
    "    best_threshold = 0\n",
    "    best_score = 0\n",
    "    for threshold in tqdm([i * 0.01 for i in range(100)]):\n",
    "        score = f1_score(y_true=y_true, y_pred=y_proba > threshold)\n",
    "        if score > best_score:\n",
    "            best_threshold = threshold\n",
    "            best_score = score\n",
    "    search_result = {'threshold': best_threshold, 'f1': best_score}\n",
    "    return search_result\n",
    "'''\n",
    "\n",
    "def threshold_search(y_true, y_proba, plot=False):\n",
    "    precision, recall, thresholds = precision_recall_curve(y_true, y_proba)\n",
    "    thresholds = np.append(thresholds, 1.001) \n",
    "    F = 2 / (1/precision + 1/recall)\n",
    "    best_score = np.max(F)\n",
    "    best_th = thresholds[np.argmax(F)]\n",
    "    if plot:\n",
    "        plt.plot(thresholds, F, '-b')\n",
    "        plt.plot([best_th], [best_score], '*r')\n",
    "        plt.show()\n",
    "    search_result = {'threshold': best_th , 'f1': best_score}\n",
    "    return search_result "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "_uuid": "923d831aa7817949571e233d60bd18e9ae460bdf"
   },
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self,dataset):\n",
    "        self.dataset = dataset\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        data, target = self.dataset[index]\n",
    "\n",
    "        return data, target, index\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "_uuid": "b76e20c70563948463fb750e40bf0b90af060b9b"
   },
   "outputs": [],
   "source": [
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        \n",
    "        hidden_size = 120\n",
    "        \n",
    "        self.embedding = nn.Embedding(max_features, embed_size*2)\n",
    "        self.embedding.weight = nn.Parameter(torch.tensor(embedding_matrix, dtype=torch.float32))\n",
    "        self.embedding.weight.requires_grad = False\n",
    "\n",
    "        self.embedding_dropout = nn.Dropout2d(0.1)\n",
    "        self.lstm = nn.LSTM(embed_size*2, hidden_size, bidirectional=True, batch_first=True)\n",
    "        #for weight in self.lstm.parameters():##\n",
    "            #if len(weight.size()) > 1:\n",
    "                #torch.nn.init.xavier_normal_(weight.data) ##\n",
    "        self.gru = nn.GRU(hidden_size*2, hidden_size, bidirectional=True, batch_first=True)\n",
    "        #for weight in self.gru.parameters():##\n",
    "            #if len(weight.size()) > 1:##\n",
    "                #torch.nn.init.xavier_normal_(weight.data) ##orthogonal_\n",
    "        \n",
    "        self.lstm_attention = Attention(hidden_size*2, maxlen)\n",
    "        self.gru_attention = Attention(hidden_size*2, maxlen)\n",
    "        \n",
    "        self.linear = nn.Linear(10*hidden_size+10, 16) #\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.batchnorm = nn.BatchNorm1d(16)##\n",
    "        self.out = nn.Linear(16, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        h_embedding = self.embedding(x[0])#\n",
    "        h_embedding = torch.squeeze(self.embedding_dropout(torch.unsqueeze(h_embedding, 0)))\n",
    "        \n",
    "        h_lstm, _ = self.lstm(h_embedding)\n",
    "        h_gru, _ = self.gru(h_lstm)\n",
    "        \n",
    "        h_lstm_atten = self.lstm_attention(h_lstm)\n",
    "        h_gru_atten = self.gru_attention(h_gru)\n",
    "        \n",
    "        avg_pool = torch.mean(h_gru, 1)\n",
    "        max_pool, _ = torch.max(h_gru, 1)\n",
    "        m2_pool = torch.topk(h_gru, 2, dim=1)[0][:,1]\n",
    "        f = torch.tensor(x[1], dtype=torch.float).cuda()##\n",
    "        \n",
    "        conc = torch.cat((h_lstm_atten, h_gru_atten, avg_pool, max_pool, m2_pool, f), 1)#\n",
    "        conc = self.relu(self.linear(conc))\n",
    "        conc = self.dropout(conc)\n",
    "        conc = self.batchnorm(conc)##\n",
    "        out = self.out(conc)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "_uuid": "d1abad523947318ed498a63d5021eec664f62f92"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n",
      "Epoch 1/4 \t loss=648.1626 \t val_loss=221.9109 \t time=292.91s\n",
      "Epoch 2/4 \t loss=231.9831 \t val_loss=197.8455 \t time=294.53s\n",
      "Epoch 3/4 \t loss=214.9310 \t val_loss=203.8569 \t time=294.82s\n",
      "Epoch 4/4 \t loss=203.9398 \t val_loss=194.8260 \t time=292.76s\n",
      "Fold 2\n",
      "Epoch 1/4 \t loss=649.2590 \t val_loss=230.6950 \t time=294.46s\n",
      "Epoch 2/4 \t loss=231.9146 \t val_loss=200.9555 \t time=293.92s\n",
      "Epoch 3/4 \t loss=216.1642 \t val_loss=202.2483 \t time=294.00s\n",
      "Epoch 4/4 \t loss=205.1613 \t val_loss=195.7174 \t time=294.67s\n",
      "Fold 3\n",
      "Epoch 1/4 \t loss=604.9511 \t val_loss=241.4589 \t time=294.74s\n",
      "Epoch 2/4 \t loss=234.9155 \t val_loss=203.0195 \t time=294.00s\n",
      "Epoch 3/4 \t loss=219.5371 \t val_loss=207.6238 \t time=295.03s\n",
      "Epoch 4/4 \t loss=209.7984 \t val_loss=196.3482 \t time=294.26s\n",
      "Fold 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:2: RuntimeWarning: overflow encountered in exp\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4 \t loss=626.1130 \t val_loss=224.9655 \t time=294.90s\n",
      "Epoch 2/4 \t loss=232.3809 \t val_loss=202.7327 \t time=295.45s\n",
      "Epoch 3/4 \t loss=217.1527 \t val_loss=204.8001 \t time=294.41s\n",
      "Epoch 4/4 \t loss=206.3008 \t val_loss=198.6197 \t time=293.85s\n",
      "Fold 5\n",
      "Epoch 1/4 \t loss=595.7119 \t val_loss=218.8594 \t time=294.73s\n",
      "Epoch 2/4 \t loss=233.5230 \t val_loss=198.5901 \t time=294.28s\n",
      "Epoch 3/4 \t loss=216.7403 \t val_loss=200.9458 \t time=293.92s\n",
      "Epoch 4/4 \t loss=205.7086 \t val_loss=193.1808 \t time=293.78s\n"
     ]
    }
   ],
   "source": [
    "train_preds = np.zeros((len(train_X)))\n",
    "test_preds = np.zeros((len(test_X)))\n",
    "features = np.array(features)##\n",
    "\n",
    "seed_torch(SEED)\n",
    "\n",
    "x_test_cuda = torch.tensor(test_X, dtype=torch.long).cuda()\n",
    "test = torch.utils.data.TensorDataset(x_test_cuda)\n",
    "test_loader = torch.utils.data.DataLoader(test, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "#class_weight = torch.FloatTensor([2.5]).cuda()##\n",
    "\n",
    "for i, (train_idx, valid_idx) in enumerate(splits):\n",
    "    x_train_fold = torch.tensor(train_X[train_idx], dtype=torch.long).cuda()\n",
    "    y_train_fold = torch.tensor(train_y[train_idx, np.newaxis], dtype=torch.float32).cuda()\n",
    "    kfold_X_features = features[train_idx.astype(int)]##\n",
    "    kfold_X_valid_features = features[valid_idx.astype(int)]##\n",
    "    x_val_fold = torch.tensor(train_X[valid_idx], dtype=torch.long).cuda()\n",
    "    y_val_fold = torch.tensor(train_y[valid_idx, np.newaxis], dtype=torch.float32).cuda()\n",
    "    \n",
    "    model = NeuralNet()\n",
    "    model.cuda()\n",
    "    \n",
    "    loss_fn = torch.nn.BCEWithLogitsLoss(reduction=\"sum\").cuda()# pos_weight , pos_weight=class_weight\n",
    "    #optimizer = torch.optim.Adam(model.parameters(), lr=2.4e-3)#, weight_decay=1e-6\n",
    "    #scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[3, 4], gamma=0.1) ##\n",
    "    \n",
    "    ################################################################################################\n",
    "    step_size = 256*2\n",
    "    base_lr, max_lr = 0.00023, 0.0024   \n",
    "    optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), \n",
    "                             lr=max_lr)\n",
    "    \n",
    "    scheduler = CyclicLR(optimizer, base_lr=base_lr, max_lr=max_lr,\n",
    "               step_size=step_size, mode='exp_range',\n",
    "               gamma=0.99994)\n",
    "    ################################################################################################\n",
    "    \n",
    "    train = torch.utils.data.TensorDataset(x_train_fold, y_train_fold)\n",
    "    valid = torch.utils.data.TensorDataset(x_val_fold, y_val_fold)\n",
    "\n",
    "    train = MyDataset(train)##\n",
    "    valid = MyDataset(valid)##\n",
    "    \n",
    "    train_loader = torch.utils.data.DataLoader(train, batch_size=batch_size, shuffle=True)\n",
    "    valid_loader = torch.utils.data.DataLoader(valid, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    print('Fold {}'.format(i + 1))\n",
    "    \n",
    "    for epoch in range(train_epochs):\n",
    "        start_time = time.time()\n",
    "        #scheduler.step() ##\n",
    "        model.train()\n",
    "        avg_loss = 0.\n",
    "        for i, (x_batch, y_batch, index) in enumerate(train_loader):\n",
    "        #for x_batch, y_batch in tqdm(train_loader, disable=True):\n",
    "            f = kfold_X_features[index]##\n",
    "            y_pred = model([x_batch,f])\n",
    "            #y_pred = model(x_batch)\n",
    "            if scheduler:\n",
    "                scheduler.batch_step()\n",
    "                \n",
    "            loss = loss_fn(y_pred, y_batch)\n",
    "            optimizer.zero_grad() \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            avg_loss += loss.item() / len(train_loader)\n",
    "        \n",
    "        model.eval()\n",
    "        valid_preds_fold = np.zeros((x_val_fold.size(0)))\n",
    "        test_preds_fold = np.zeros(len(test_X))\n",
    "        avg_val_loss = 0.\n",
    "        for i, (x_batch, y_batch, index) in enumerate(valid_loader):\n",
    "        #for i, (x_batch, y_batch) in enumerate(valid_loader):\n",
    "            f = kfold_X_valid_features[index]##\n",
    "            y_pred = model([x_batch,f]).detach()\n",
    "            #y_pred = model(x_batch).detach()\n",
    "            avg_val_loss += loss_fn(y_pred, y_batch).item() / len(valid_loader)\n",
    "            valid_preds_fold[i * batch_size:(i+1) * batch_size] = sigmoid(y_pred.cpu().numpy())[:, 0]\n",
    "        \n",
    "        elapsed_time = time.time() - start_time \n",
    "        print('Epoch {}/{} \\t loss={:.4f} \\t val_loss={:.4f} \\t time={:.2f}s'.format(\n",
    "            epoch + 1, train_epochs, avg_loss, avg_val_loss, elapsed_time))\n",
    "        \n",
    "    for i, (x_batch,) in enumerate(test_loader):\n",
    "    #for i, (x_batch,) in enumerate(test_loader):\n",
    "        f = test_features[i * batch_size:(i+1) * batch_size]##\n",
    "        y_pred = model([x_batch,f]).detach()\n",
    "        #y_pred = model(x_batch).detach()\n",
    "\n",
    "        test_preds_fold[i * batch_size:(i+1) * batch_size] = sigmoid(y_pred.cpu().numpy())[:, 0]\n",
    "\n",
    "    train_preds[valid_idx] = valid_preds_fold\n",
    "    test_preds += test_preds_fold / len(splits)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "_uuid": "7dafbc4af2fe1d93a5cbe2c014ea234e7e32f731"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:17: RuntimeWarning: divide by zero encountered in true_divide\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'threshold': 0.3100835680961609, 'f1': 0.6934346861092263}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_result = threshold_search(train_y, train_preds)\n",
    "search_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "_uuid": "a72e5eabf7496240a3e71536b1e788d8eaed3a88"
   },
   "outputs": [],
   "source": [
    "sub = pd.read_csv(\"../input/test.csv\")\n",
    "sub = sub[['qid']]\n",
    "##test_preds = test_preds[:56370]##压力测试\n",
    "sub['prediction'] = test_preds > search_result['threshold']\n",
    "sub.to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "_uuid": "82a6aa53d956ca2ab687188b5c5e280c58d6d817"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "qid,prediction\r\n",
      "00014894849d00ba98a9,False\r\n",
      "000156468431f09b3cae,False\r\n",
      "000227734433360e1aae,False\r\n",
      "0005e06fbe3045bd2a92,False\r\n",
      "00068a0f7f41f50fc399,False\r\n",
      "000a2d30e3ffd70c070d,False\r\n",
      "000b67672ec9622ff761,False\r\n",
      "000b7fb1146d712c1105,False\r\n",
      "000d665a8ddc426a1907,False\r\n"
     ]
    }
   ],
   "source": [
    "!head submission.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "_uuid": "0bb8fec17dfdb4252cfba54449bd547fbc7f40e4"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "_uuid": "079fd36b96c88307141bf30b27f5388a7c3f7ef7"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "_uuid": "79649f85a8ed82e85248b733d8f95a02edade9f4"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "_uuid": "7ec18bd86059764320a5c7167da6671e829569e4"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "_uuid": "6b19f66c0339c28b336937e278f16ce7e97660a3"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
